{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "batch_size=256\n",
    "#read data\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function\n",
    "#先使用Flatten层将输入数据转成 batch_size x ? 的矩阵，然后输入到10个输出节点的全连接层。\n",
    "\n",
    "from mxnet import gluon\n",
    "\n",
    "net=gluon.nn.Sequential() \n",
    "#构建模型最简单的办法是利用Sequential来所有层串起来。\n",
    "#输入数据之后，Sequential会依次执行每一层，并将前一层的输出，作为输入提供给后面的层。\n",
    "#创建一个胶水\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(256,activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "net.initialize()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define softmax function and cross entropy cost function\n",
    "softmax_cross_entropy=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "#initialization\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,Loss 0.717606,Train acc0.737280, Test acc 0.742188\n",
      "Epoch 1,Loss 0.467884,Train acc0.827491, Test acc 0.845553\n",
      "Epoch 2,Loss 0.413429,Train acc0.847923, Test acc 0.857372\n",
      "Epoch 3,Loss 0.381127,Train acc0.859609, Test acc 0.858073\n",
      "Epoch 4,Loss 0.355986,Train acc0.869074, Test acc 0.873998\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "from mxnet import autograd\n",
    "for epoch in range(5):\n",
    "    train_loss=0.\n",
    "    train_acc=0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output=net(data)\n",
    "            loss=softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss+=nd.mean(loss).asscalar()\n",
    "        train_acc+=utils.accuracy(output, label)\n",
    "        \n",
    "    test_acc=utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d,Loss %f,Train acc%f, Test acc %f\"%(\n",
    "    epoch,train_loss/len(train_data), train_acc/len(train_data), test_acc)\n",
    "    )\n",
    "  \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
