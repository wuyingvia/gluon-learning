{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#卷积神经网络是指主要由卷积层构成的神经网络\n",
    "#解决：模型计算量大、在图片相近的像素在向量里的表示可能相差很远\n",
    "#相关术语：卷积层kernel或者filter （代表相应位置相乘的和）pooling(池化层)\n",
    "#nd.Convolution实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]\n",
      "   [ 6.  7.  8.]]]]\n",
      "<NDArray 1x1x3x3 @cpu(0)> \n",
      "\n",
      "weight: \n",
      "[[[[ 0.  1.]\n",
      "   [ 2.  3.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)> \n",
      "\n",
      "bias: \n",
      "[ 1.]\n",
      "<NDArray 1 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[ 20.  26.]\n",
      "   [ 38.  44.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import ndarray as nd\n",
    "\n",
    "#输入：单通道；输出：单通道\n",
    "\n",
    "# 输入输出数据格式是 batch x channel x height x width，这里batch和channel都是1\n",
    "#weight: (num_filter, channel, kernel[0], kernel[1])\n",
    "#权重格式是 output_channels x in_channels x height x width\n",
    "w = nd.arange(4).reshape((1,1,2,2))\n",
    "#其实我可以这样理解，1代表batch数值，可以代表索引数，第几个包包\n",
    "#第二个是channal数，可以理解为维度，我可以print（height x width）维的矩阵\n",
    "b = nd.array([1])\n",
    "data = nd.arange(9).reshape((1,1,3,3))\n",
    "out=nd.Convolution(data, w, b, kernel=w.shape[2:], num_filter=w.shape[1])\n",
    "#kernel=w.shape[2:]没有看懂这个意思\n",
    "#num_filter=w.shape[1]卷积核的数量是w的一维数值\n",
    "\n",
    "print('input:', data, '\\n\\nweight:', w, '\\n\\nbias:', b, '\\n\\noutput:', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]\n",
      "   [ 6.  7.  8.]]]]\n",
      "<NDArray 1x1x3x3 @cpu(0)> \n",
      "\n",
      "weight: \n",
      "[[[[ 0.  1.]\n",
      "   [ 2.  3.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)> \n",
      "\n",
      "bias: \n",
      "[ 1.]\n",
      "<NDArray 1 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[  1.   9.]\n",
      "   [ 22.  44.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#输入：单通道；输出：单通道； 步长：2\n",
    "out = nd.Convolution(data, w, b, kernel=w.shape[2:], num_filter=w.shape[1],\n",
    "                     stride=(2,2), pad=(1,1))\n",
    "\n",
    "print('input:', data, '\\n\\nweight:', w, '\\n\\nbias:', b, '\\n\\noutput:', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[  0.   1.   2.]\n",
      "   [  3.   4.   5.]\n",
      "   [  6.   7.   8.]]\n",
      "\n",
      "  [[  9.  10.  11.]\n",
      "   [ 12.  13.  14.]\n",
      "   [ 15.  16.  17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      "weight: \n",
      "[[[[ 0.  1.]\n",
      "   [ 2.  3.]]\n",
      "\n",
      "  [[ 4.  5.]\n",
      "   [ 6.  7.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)> \n",
      "\n",
      "bias: \n",
      "[ 1.]\n",
      "<NDArray 1 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[ 269.  297.]\n",
      "   [ 353.  381.]]]]\n",
      "<NDArray 1x1x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#输入：多通道；输出：单通道\n",
    "w = nd.arange(8).reshape((1,2,2,2))\n",
    "data = nd.arange(18).reshape((1,2,3,3))\n",
    "\n",
    "out=nd.Convolution(data,w,b,kernel=w.shape[2:],num_filter=w.shape[0])\n",
    "\n",
    "print('input:',data,'\\n\\nweight:',w,'\\n\\nbias:', b, '\\n\\noutput:', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[[[  0.   1.   2.]\n",
      "   [  3.   4.   5.]\n",
      "   [  6.   7.   8.]]\n",
      "\n",
      "  [[  9.  10.  11.]\n",
      "   [ 12.  13.  14.]\n",
      "   [ 15.  16.  17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      "weight: \n",
      "[[[[  0.   1.]\n",
      "   [  2.   3.]]\n",
      "\n",
      "  [[  4.   5.]\n",
      "   [  6.   7.]]]\n",
      "\n",
      "\n",
      " [[[  8.   9.]\n",
      "   [ 10.  11.]]\n",
      "\n",
      "  [[ 12.  13.]\n",
      "   [ 14.  15.]]]]\n",
      "<NDArray 2x2x2x2 @cpu(0)> \n",
      "\n",
      "bias: \n",
      "[ 1.  2.]\n",
      "<NDArray 2 @cpu(0)> \n",
      "\n",
      "output: \n",
      "[[[[  269.   297.]\n",
      "   [  353.   381.]]\n",
      "\n",
      "  [[  686.   778.]\n",
      "   [  962.  1054.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#输入：多通道 输出：多通道\n",
    "data=nd.arange(18).reshape((1,2,3,3))\n",
    "w=nd.arange(16).reshape((2,2,2,2))\n",
    "b=nd.array([1,2])\n",
    "\n",
    "out=nd.Convolution(data,w,b,kernel=w.shape[2:],num_filter=w.shape[0])\n",
    "print('input:', data, '\\n\\nweight:', w, '\\n\\nbias:', b, '\\n\\noutput:', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[[[  0.   1.   2.]\n",
      "   [  3.   4.   5.]\n",
      "   [  6.   7.   8.]]\n",
      "\n",
      "  [[  9.  10.  11.]\n",
      "   [ 12.  13.  14.]\n",
      "   [ 15.  16.  17.]]]]\n",
      "<NDArray 1x2x3x3 @cpu(0)> \n",
      "\n",
      "max pooling: \n",
      "[[[[  4.   5.]\n",
      "   [  7.   8.]]\n",
      "\n",
      "  [[ 13.  14.]\n",
      "   [ 16.  17.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)> \n",
      "\n",
      "avg pooling: \n",
      "[[[[  2.   3.]\n",
      "   [  5.   6.]]\n",
      "\n",
      "  [[ 11.  12.]\n",
      "   [ 14.  15.]]]]\n",
      "<NDArray 1x2x2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "data = nd.arange(18).reshape((1,2,3,3))\n",
    "\n",
    "max_pool = nd.Pooling(data=data, pool_type=\"max\", kernel=(2,2))\n",
    "avg_pool = nd.Pooling(data=data, pool_type=\"avg\", kernel=(2,2))\n",
    "\n",
    "print('data:', data, '\\n\\nmax pooling:', max_pool, '\\n\\navg pooling:', avg_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mnist data\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils import load_data_fashion_mnist\n",
    "\n",
    "batch_size=256\n",
    "train_data, test_data = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "import mxnet as mx\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "from mxnet import autograd\n",
    "#the first layer\n",
    "weight_scale = .01\n",
    "\n",
    "# output channels = 20, kernel = (5,5)\n",
    "W1 = nd.random_normal(shape=(20,1,5,5), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.zeros(W1.shape[0], ctx=ctx)\n",
    "\n",
    "# output channels = 50, kernel = (3,3)\n",
    "W2 = nd.random_normal(shape=(50,20,3,3), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.zeros(W2.shape[0], ctx=ctx)\n",
    "\n",
    "# output dim = 128\n",
    "W3 = nd.random_normal(shape=(1250, 128), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.zeros(W3.shape[1], ctx=ctx)\n",
    "\n",
    "# output dim = 10\n",
    "W4 = nd.random_normal(shape=(W3.shape[1], 10), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.zeros(W4.shape[1], ctx=ctx)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function\n",
    "#there are three basic construction in convolutionnal layer\n",
    "#conv+activation+pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X,verbose=False):# what means verbose? verbose 相当于一个开关，如果verbose=true，我就相当于打开开关，下面的程序开始执行\n",
    "    X = X.as_in_context(W1.context)#我也看不懂为什么X这样定义，X输入进去的是图片？\n",
    "    #the first convolutional layer\n",
    "    h1_conv=nd.Convolution(\n",
    "    data=X,weight=w1,bias=b1, kernel=w1.shape[2:], num_filter=W1.shape[0])\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1=nd.Pooling(\n",
    "    data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    #the second convolutional layer\n",
    "    h2_conv=nd.Convolution(\n",
    "    data=h1,weight=w2,bias=b2, kernel=w2.shape[2:], num_filter=w2.shape[0])\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2=nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    #the first connection layer 全连接层好像就是普通的线性处理，那前面的卷积层就应该是对图像进行降维处理\n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    #the second connection layer\n",
    "    h4_linear = nd.dot(h3, W4) + b4\n",
    "    h4 = nd.relu(h4_linear)\n",
    "    \n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.shape)\n",
    "        print('2nd conv block:', h2.shape)\n",
    "        print('1st dense:', h3.shape)\n",
    "        print('2nd dense:', h4_linear.shape)\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.060994, Train acc 0.595670, Test acc 0.697716\n",
      "Epoch 1. Loss: 0.623971, Train acc 0.753990, Test acc 0.784054\n",
      "Epoch 2. Loss: 0.546025, Train acc 0.789413, Test acc 0.803385\n",
      "Epoch 3. Loss: 0.499387, Train acc 0.812366, Test acc 0.770232\n",
      "Epoch 4. Loss: 0.469481, Train acc 0.825020, Test acc 0.844651\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "from mxnet import autograd as autograd\n",
    "from utils import SGD, accuracy, evaluate_accuracy\n",
    "from mxnet import gluon\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "learning_rate = .2\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为什么Epoch 3. Loss: 0.499387, Train acc 0.812366, Test acc 0.770232\n",
    "#Test acc精度会下降了一下？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
